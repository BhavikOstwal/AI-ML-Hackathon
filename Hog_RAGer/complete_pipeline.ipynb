{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Hog RAGer**\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BhavikOstwal/AI-ML-Hackathon/blob/main/Hog_RAGer/complete_pipeline.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "-mpKsNELW3KS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the corpus and train datasets\n",
        "with open(\"data/corpus.json\", \"r\") as corpus_file:\n",
        "    corpus = json.load(corpus_file)\n",
        "\n",
        "with open(\"data/train.json\", \"r\") as train_file:\n",
        "    train_data = json.load(train_file)\n",
        "\n",
        "# Extract text and metadata from corpus for TF-IDF\n",
        "corpus_texts = [doc['body'] for doc in corpus]\n",
        "corpus_metadata = [{key: doc[key] for key in ['title', 'author', 'url', 'source', 'category', 'published_at']} for doc in corpus]\n",
        "\n",
        "# Tokenize and fit TF-IDF on the corpus\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "corpus_tfidf = vectorizer.fit_transform(corpus_texts)\n",
        "\n",
        "# Load pre-trained model and tokenizer for answer generation (e.g., t5-small or similar open-source model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to match query to corpus and retrieve evidence\n",
        "def retrieve_evidence(query):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    cosine_similarities = cosine_similarity(query_tfidf, corpus_tfidf).flatten()\n",
        "\n",
        "    # Get indices of top matches\n",
        "    top_indices = cosine_similarities.argsort()[-3:][::-1]\n",
        "\n",
        "    evidence_list = []\n",
        "    for idx in top_indices:\n",
        "        evidence = {\n",
        "            \"title\": corpus[idx]['title'],\n",
        "            \"author\": corpus[idx]['author'],\n",
        "            \"url\": corpus[idx]['url'],\n",
        "            \"source\": corpus[idx]['source'],\n",
        "            \"category\": corpus[idx]['category'],\n",
        "            \"published_at\": corpus[idx]['published_at'],\n",
        "            \"fact\": corpus[idx]['body'][:150]  # Just taking first 150 chars as a sample fact\n",
        "        }\n",
        "        evidence_list.append(evidence)\n",
        "\n",
        "    return evidence_list, [corpus_texts[idx] for idx in top_indices]\n",
        "\n",
        "# Function to generate answer from top evidence using a pre-trained model\n",
        "def generate_answer(query, evidence_texts):\n",
        "    # Concatenate the relevant documents as context\n",
        "    context = \" \".join(evidence_texts)\n",
        "\n",
        "    # Prepare input for the model by combining query and context\n",
        "    input_text = f\"question: {query} context: {context}\"\n",
        "\n",
        "    # Tokenize and generate the answer using the model\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output tokens to get the final answer\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Example function to process a query\n",
        "def process_query(query):\n",
        "    # Retrieve evidence from the corpus\n",
        "    evidence_list, evidence_texts = retrieve_evidence(query)\n",
        "\n",
        "    # Generate answer using the model\n",
        "    answer = generate_answer(query, evidence_texts)\n",
        "\n",
        "    # Structure the result\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,  # The generated answer from the model\n",
        "        \"question_type\": \"inference_query\",  # Assuming all questions are inference queries\n",
        "        \"evidence_list\": evidence_list  # Evidence used for generating the answer\n",
        "    }\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t1ufyUE-Sbl6"
      },
      "outputs": [],
      "source": [
        "with open('out/output.json', 'w') as output_file:\n",
        "  for i in range(len(train_data)):\n",
        "    test_query = train_data[i]['query']\n",
        "    output = process_query(test_query)\n",
        "\n",
        "    # Print output in required format\n",
        "    # print(json.dumps(output, indent=2))\n",
        "\n",
        "    # You can now write the results to a JSON file\n",
        "    json.dump(output, output_file, indent=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
